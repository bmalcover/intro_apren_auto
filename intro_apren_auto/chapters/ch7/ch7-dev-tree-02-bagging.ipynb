{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas de Agregación\n",
    "\n",
    "Tal como se describe en la introducción del capítulo _Bagging_, _random\n",
    "forests_ y _boosting_ utilizan los árboles de decisión como las piezas\n",
    "elementales para construir mejores modelos de predicción.\n",
    "\n",
    "### Bagging\n",
    "**NOTA:** Traduir bagging? Wikipedia ho tradueix com Agregación de bootstrap o\n",
    "empaquetado\n",
    "\n",
    "\n",
    "Los árboles de decisión sufren de una gran varianza, esto significa que si\n",
    "dividimos los datos de entrenamiento en dos partes de manera aleatoria, y\n",
    "construimos un árbol de decisión para cada una de las dos mitades, los\n",
    "árboles resultantes que obtenemos podrían ser muy diferentes. Un\n",
    "procedimiento  con baja varianza producirá resultados similares si se aplica\n",
    "a distintos conjuntos de datos.\n",
    "\n",
    "La técnica que aquí se presenta, __bagging__, es un procedimiento diseñado para\n",
    "reducir la varianza de un método de aprendizaje estadístico. Dado un\n",
    "conjunto de $n$ observaciones independientes $Z_1, \\ldots, Z_n$ cada una de\n",
    "ellas con su propia varianza $ \\sigma^2$, la varianza de la media de estas\n",
    "observaciones $\\overline{Z}$ es  $ \\frac{\\sigma^2}{n}$. Por lo\n",
    "tanto, una forma natural de reducir la varianza y, por lo tanto, aumentar la\n",
    "precisión de la predicción de un método de aprendizaje es construir\n",
    "diferentes conjuntos de entrenamiento, construir un modelo de\n",
    "predicción separado usando cada uno de los conjuntos y realizar el promedio\n",
    "de las predicciones resultantes.\n",
    "\n",
    "Desgraciadamente, normalmente no tenemos suficientes datos para crear\n",
    "diferentes conjuntos de entrenamiento. Es por este motivo que se aplica la\n",
    "técnica de __bootstrap__, simularemos los diferentes conjuntos de\n",
    "entrenamiento necesarios para construir diferentes árboles y reducir la\n",
    "varianza del clasificador. Esto se consigue extrayendo repetidos subconjuntos\n",
    "de la muestra original. Estos subconjuntos muestrales deben extraerse\n",
    "utilizando  un muestreo con reposición, de tal forma que algunos elementos no\n",
    "serán  seleccionados y otros lo podrán ser más de una vez en cada muestreo.\n",
    "(_refereciar al Hasting capítol 5_)\n",
    "\n",
    "**TODO**: gràfic mostrant bootstrap\n",
    "\n",
    "Para aplicar la técnica de _bagging_ construiremos un conjunto de árboles de\n",
    "regresión usando diferentes conjuntos de entrenamiento con la técnica\n",
    "explicada anteriormente y daremos como resultado final el promedio de las\n",
    "predicciones individuales de cada uno de los árboles que forman el conjunto.\n",
    "(_Estos árboles crecen profundamente y  no se podan_) . Al promediar el\n",
    "resultado de cada uno de estos árboles evidentemente, se consigue reducir la\n",
    "varianza. Se ha demostrado que el _bagging_ proporciona grandes mejoras  en la\n",
    "precisión de los resultado al combinar un gran número de árboles en un\n",
    "solo procedimiento.\n",
    "\n",
    "Hasta ahora, hemos descrito el procedimiento de _bagging_ en  el caso de\n",
    "necesitar solventar un problema aplicando una regresión. Esta técnica también\n",
    "es aplicable a problemas de clasificación, en este caso se\n",
    "toma la clase que más clasificadores (árboles) hayan predicho.\n",
    "\n",
    "**NOTA:** Integrar això, potser s'hauria de'eXplicar en la secció anterior.\n",
    "Bias-variance tradeoff\n",
    "\n",
    "_Según lo explicado anteriormente, sabemos que cada árbol\n",
    "individual tiene una alta varianza, pero un bajo sesgo (xq son profundos). Al\n",
    "obtener información agregada del conjunto  árboles se reduce la varianza._\n",
    "\n",
    "\n",
    "#### Cálculo de el error (_out-of-bag_)\n",
    "\n",
    "\n",
    "#### Un ejemplo de _bagging_\n",
    "\n",
    "Como hemos explicado anteriormente esta técnica es aplicable a diferentes\n",
    "métodoes de aprendizaje. En el paquete _sklearn.ensemble_ de Scikit\n",
    "encontramos las clases _BaggingClassifier_ y _BaggingRegressor_ que usan los\n",
    "árboles de decisión como técnica por defecto.\n",
    "\n",
    "Vamos a ver un ejemplo de regresión con esta técnica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_regression(n_samples=50, n_features=1,\n",
    "                       n_informative=1, n_targets=1, noise=10,\n",
    "                       random_state=0, shuffle=True)\n",
    "\n",
    "elem = np.linspace(np.min(X),np.max(X),150).reshape(-1, 1)\n",
    "\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=33)\n",
    "regressor.fit(X, y)\n",
    "\n",
    "predicciones = regressor.predict(elem)\n",
    "\n",
    "regr = BaggingRegressor(n_estimators=100, random_state=0)\n",
    "regr.fit(X, y)\n",
    "\n",
    "y_predict= regr.predict(elem)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plt.scatter(X, y);\n",
    "#plt.plot(elem, y_predict,label=\"bagging\");\n",
    "#plt.plot(elem, predicciones, label=\"regressor tree\")\n",
    "#plt.legend()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest\n",
    "\n",
    "Los _random forests_ son una técnica que al igual que la técnica de\n",
    "_bagging_ consiste en la agregación de diversos árboles de decisión\n",
    "para proporcionar una decisión conjunta creando diferentes conjuntos de\n",
    "entrenamiento  mediante _bootstraping_. Esta nueva técnica que presentamos\n",
    "consigue una mejora de rendimiento respecto a la técnica  de _bagging_.\n",
    "\n",
    "La particularidad que diferencia a los _random forest_ es que al construir\n",
    "cada  uno de los árboles de decisión, cada vez que se realiza una división de\n",
    "un  nodo en un árbol, se elige de manera  aleatoria un subconjunto, $m$, de\n",
    "características. Este nueva división solo podrá ser realizada usando una\n",
    "característica que pertenezca a $m$. Cada vez que se realiza una nueva\n",
    "división se vuelve a generar este conjunto, el valor por defecto de $m \\simeq\n",
    "\\sqrt{p}$ donde $p$ es el número total de características.\n",
    "\n",
    "\n",
    "En resumen, al construir un _random forest_, en cada división del árbol,\n",
    "el algoritmo no considerará la mayoría de las características disponibles.\n",
    "Esto permite construir árboles que serán muy diferentes entre si y permite\n",
    "evitar que existan características que sean dominantes. Una de las\n",
    "problemáticas que surgen del uso de la técnica de _bagging_ es la similitud\n",
    "de todos los árboles construidos. Desafortunadamente, promediar muchos valores\n",
    "altamente correlacionados no conduce a una gran reducción de la varianza como\n",
    "promediar muchos valores no correlacionados. Esto significa que la técnica\n",
    "de _bagging_ no proporcionará una gran reducción de la varianza comparando\n",
    "con la que tenemos con un solo árbol. Los _random forest_ estan diseñados\n",
    "para  solventar esta problemática al forzar cada división a considerar un\n",
    "subconjunto de las características.\n",
    "\n",
    "La principal diferencia entre _bagging_ y _random forest_ es  la elección del\n",
    "tamaño del subconjunto del predictor. Por ejemplo, si un  _random forest_ se\n",
    "construye usando  $m = p$, esto equivale simplemente a realizar la técnica de\n",
    "_bagging_.\n",
    "\n",
    "\n",
    "### Boosting\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Determinar la importancia de las características\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Un ejemplo completo\n",
    "\n",
    "**NOTA:** aquí la meva idea seria fer un exemple de classificació comparant\n",
    "arbres simples amb alguna de les tècniques anteriors. Fer selecció dels\n",
    "millors parametres i mostrar com canvia el resultat amb el nombre d'estimadors.\n",
    "\n",
    "## Resumen\n",
    "\n",
    "\n",
    "* Comentar que tant en els arbres individuals com en les tècnicques\n",
    "d'agregació ne le cas de classif es pot donar %\n",
    "\n",
    "* En resumen, el  promedio de un conjunto de observaciones permite reducir la\n",
    "varianza.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Ejercicios"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}