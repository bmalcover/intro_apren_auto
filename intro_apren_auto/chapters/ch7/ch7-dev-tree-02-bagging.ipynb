{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas de Agregación\n",
    "\n",
    "Tal como se describe en la introducción del capítulo _Bagging_, _random\n",
    "forests_ y _boosting_ utilizan los árboles de decisión como las piezas\n",
    "elementales para construir mejores modelos de predicción.\n",
    "\n",
    "\n",
    "**COPIAT**\n",
    "\n",
    "Al igual que todo modelo estadístico, los árboles de predicción sufren el\n",
    "problema del equilibrio bias-varianza. El término bias hace referencia a\n",
    "cuánto  se alejan en promedio las predicciones de un modelo respecto a los\n",
    "valores reales, es decir, cómo de bien se aproxima el modelo a la relación\n",
    "real  entre las variables. La varianza hace referencia a cuánto varía el\n",
    "modelo  dependiendo de la muestra empleada en el entrenamiento. A medida que\n",
    "se  aumenta la complejidad de un modelo, se dispone de mayor flexibilidad\n",
    "para  adaptarlo a las observaciones, reduciendo así el bias y mejorando su\n",
    "capacidad predictiva. Sin embargo, alcanzado un determinado grado de\n",
    "flexibilidad, aparece el problema de overfitting, el modelo se ajusta tanto a\n",
    "los datos de entrenamiento que es incapaz de predecir correctamente nuevas\n",
    "observaciones. El mejor modelo es aquel que consigue un equilibro óptimo\n",
    "entre bias y varianza.\n",
    "\n",
    "¿Cómo se controlan el bias y varianza en los árboles de predicción? Por lo\n",
    "general, los árboles pequeños (pocas ramificaciones) no representarán bien la\n",
    "relación entre las variables, por lo que tienen alto bias, pero poca varianza\n",
    ". Los árboles grandes se ajustan mucho a los datos de entrenamiento, por lo\n",
    "que  tienen muy poco bias pero mucha varianza. Los métodos de ensemble\n",
    "abarcan  un conjunto de técnicas que combinan múltiples modelos predictivos\n",
    "para  lograr un equilibro entre bias y varianza. Aunque pueden emplearse con\n",
    " multitud de métodos de aprendizaje estadístico (K-NN, redes neuronales…),\n",
    " con  los árboles de predicción, dan muy buenos resultados\n",
    "\n",
    "**FINS AQUI**\n",
    "\n",
    "### Bagging\n",
    "**NOTA:** Traduir bagging? Wikipedia ho tradueix com Agregación de bootstrap o\n",
    "empaquetado\n",
    "\n",
    "\n",
    "Los árboles de decisión sufren de una gran varianza, esto significa que si\n",
    "dividimos los datos de entrenamiento en dos partes de manera aleatoria, y\n",
    "construimos un árbol de decisión para cada una de las dos mitades, los\n",
    "árboles resultantes que obtenemos podrían ser muy diferentes. Un\n",
    "procedimiento  con baja varianza producirá resultados similares si se aplica\n",
    "a distintos conjuntos de datos.\n",
    "\n",
    "La técnica que aquí se presenta, __bagging__, es un procedimiento diseñado para\n",
    "reducir la varianza de un método de aprendizaje estadístico. Dado un\n",
    "conjunto de $n$ observaciones independientes $Z_1, \\ldots, Z_n$ cada una de\n",
    "ellas con su propia varianza $ \\sigma^2$, la varianza de la media de estas\n",
    "observaciones $\\overline{Z}$ es  $ \\frac{\\sigma^2}{n}$. Por lo\n",
    "tanto, una forma natural de reducir la varianza y, por lo tanto, aumentar la\n",
    "precisión de la predicción de un método de aprendizaje es construir\n",
    "diferentes conjuntos de entrenamiento, construir un modelo de\n",
    "predicción separado usando cada uno de los conjuntos y realizar el promedio\n",
    "de las predicciones resultantes.\n",
    "\n",
    "Desgraciadamente, normalmente no tenemos suficientes datos para crear\n",
    "diferentes conjuntos de entrenamiento. Es por este motivo que se aplica la\n",
    "técnica de __bootstrap__, simularemos los diferentes conjuntos de\n",
    "entrenamiento necesarios para construir diferentes árboles y reducir la\n",
    "varianza del clasificador. Esto se consigue extrayendo repetidos subconjuntos\n",
    "de la muestra original. Estos subconjuntos muestrales deben extraerse\n",
    "utilizando  un muestreo con reposición, de tal forma que algunos elementos no\n",
    "serán  seleccionados y otros lo podrán ser más de una vez en cada muestreo.\n",
    "(_refereciar al Hasting capítol 5_)\n",
    "\n",
    "**TODO**: gràfic mostrant bootstrap\n",
    "\n",
    "Para aplicar la técnica de _bagging_ construiremos un conjunto de árboles de\n",
    "regresión usando diferentes conjuntos de entrenamiento con la técnica\n",
    "explicada anteriormente y daremos como resultado final el promedio de las\n",
    "predicciones individuales de cada uno de los árboles que forman el conjunto.\n",
    "(_Estos árboles crecen profundamente y  no se podan_) . Al promediar el\n",
    "resultado de cada uno de estos árboles evidentemente, se consigue reducir la\n",
    "varianza. Se ha demostrado que el _bagging_ proporciona grandes mejoras  en la\n",
    "precisión de los resultado al combinar un gran número de árboles en un\n",
    "solo procedimiento.\n",
    "\n",
    "Hasta ahora, hemos descrito el procedimiento de _bagging_ en  el caso de\n",
    "necesitar solventar un problema aplicando una regresión. Esta técnica también\n",
    "es aplicable a problemas de clasificación, en este caso se\n",
    "toma la clase que más clasificadores (árboles) hayan predicho.\n",
    "\n",
    "**NOTA:** Integrar això, potser s'hauria de'eXplicar en la secció anterior.\n",
    "Bias-variance tradeoff\n",
    "\n",
    "_Según lo explicado anteriormente, sabemos que cada árbol\n",
    "individual tiene una alta varianza, pero un bajo sesgo (xq son profundos). Al\n",
    "obtener información agregada del conjunto  árboles se reduce la varianza._\n",
    "\n",
    "\n",
    "#### Cálculo de el error (_out-of-bag_)\n",
    "\n",
    "**COPIAT**\n",
    "\n",
    "\n",
    "_Existe una manera muy sencilla de estimar el error de test en el caso de usar\n",
    " la técnica de _bagging_ sin la necesidad de realizar validación cruzada\n",
    "o crear un subconjuto de validación. Recordemos que la clave para la técnica\n",
    "de _bagging_ es que los árboles se construyen usando XXXXXXX._\n",
    "\n",
    "\n",
    "Dada la naturaleza del proceso de bagging, resulta posible estimar de forma\n",
    "directa el test error sin necesidad de recurrir a cross-validation o a un\n",
    "test  set independiente. Sin entrar en demostraciones matemáticas, el hecho\n",
    "de  que los árboles se ajusten de forma repetida empleando muestras generadas\n",
    "por bootstrapping conlleva que, en promedio, cada ajuste usa solo\n",
    "aproximadamente dos tercios de las observaciones originales. Al tercio\n",
    "restante se le llama out-of-bag (OOB). Si para cada árbol ajustado en el\n",
    "proceso de bagging se registran las observaciones empleadas, se puede\n",
    "predecir  la respuesta de la observación i haciendo uso de aquellos árboles\n",
    "en los que esa observación ha sido excluida (OOB) y promediándolos (la moda\n",
    "en el caso de los árboles de clasificación). Siguiendo este proceso, se\n",
    "pueden  obtener las predicciones para las n observaciones y con ellas\n",
    "calcular el OOB-mean square error (para regresión) o el OOB-classification\n",
    "error  (para árboles de clasificación). Como la variable respuesta de cada\n",
    "observación se predice empleando únicamente los árboles en cuyo ajuste no\n",
    "participó dicha observación, el OOB-error sirve como estimación del\n",
    "test-error.  De hecho, si el número de árboles es suficientemente alto, el\n",
    "OOB-error es prácticamente equivalente al leave-one-out cross-validation\n",
    "error.  Esta es una ventaja añadida de los métodos de bagging, ya que evita\n",
    "tener  que recurrir al proceso de cross-validation (computacionalmente\n",
    "costoso)  para la optimización de los hiperparámetros.\n",
    "\n",
    "\n",
    "#### Un ejemplo de _bagging_\n",
    "\n",
    "Como hemos explicado anteriormente esta técnica es aplicable a diferentes\n",
    "métodoes de aprendizaje. En el paquete _sklearn.ensemble_ de Scikit\n",
    "encontramos las clases _BaggingClassifier_ y _BaggingRegressor_ que usan los\n",
    "árboles de decisión como técnica por defecto.\n",
    "\n",
    "Vamos a ver un ejemplo de regresión con esta técnica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_regression(n_samples=50, n_features=1,\n",
    "                       n_informative=1, n_targets=1, noise=10,\n",
    "                       random_state=0, shuffle=True)\n",
    "\n",
    "elem = np.linspace(np.min(X),np.max(X),150).reshape(-1, 1)\n",
    "\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=33)\n",
    "regressor.fit(X, y)\n",
    "\n",
    "predicciones = regressor.predict(elem)\n",
    "\n",
    "regr = BaggingRegressor(n_estimators=100, random_state=0)\n",
    "regr.fit(X, y)\n",
    "\n",
    "y_predict= regr.predict(elem)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plt.scatter(X, y);\n",
    "#plt.plot(elem, y_predict,label=\"bagging\");\n",
    "#plt.plot(elem, predicciones, label=\"regressor tree\")\n",
    "#plt.legend()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest\n",
    "\n",
    "Los _random forests_ son una técnica que al igual que la técnica de\n",
    "_bagging_ consiste en la agregación de diversos árboles de decisión\n",
    "para proporcionar una decisión conjunta creando diferentes conjuntos de\n",
    "entrenamiento  mediante _bootstraping_. Esta nueva técnica que presentamos\n",
    "consigue una mejora de rendimiento respecto a la técnica  de _bagging_.\n",
    "\n",
    "La particularidad que diferencia a los _random forest_ es que al construir\n",
    "cada  uno de los árboles de decisión, cada vez que se realiza una división de\n",
    "un  nodo en un árbol, se elige de manera  aleatoria un subconjunto, $m$, de\n",
    "características. Este nueva división solo podrá ser realizada usando una\n",
    "característica que pertenezca a $m$. Cada vez que se realiza una nueva\n",
    "división se vuelve a generar este conjunto, el valor por defecto de $m \\simeq\n",
    "\\sqrt{p}$ donde $p$ es el número total de características.\n",
    "\n",
    "\n",
    "En resumen, al construir un _random forest_, el algoritmo no considerará la\n",
    "mayoría de las características disponibles en cada nueva división de un nodo\n",
    "del árbol. Esto permite construir árboles que serán muy diferentes entre si y\n",
    " también evitar que existan características que dominantes.\n",
    "\n",
    "Una de las problemáticas que surgen del uso de la técnica de _bagging_ es la\n",
    "similitud de todos los árboles construidos. Desafortunadamente, realizar\n",
    "promedio de muchos valores altamente correlacionados no conduce a una gran\n",
    "reducción de la varianza como promediar muchos valores no correlacionados.\n",
    "Esto significa que la técnica de _bagging_ no proporcionará una gran\n",
    "reducción de la varianza comparando con la que tenemos con un solo árbol.\n",
    "Los _random forest_ estan diseñados para  solventar esta problemática al\n",
    "forzar  cada división a considerar un subconjunto de todas las\n",
    "características disponibles.\n",
    "\n",
    "La principal diferencia entre _bagging_ y _random forest_ es  la elección del\n",
    "tamaño del subconjunto del predictor, si un  _random forest_ se\n",
    "construye usando  $m = p$, esto equivale a realizar la técnica de\n",
    "_bagging_.\n",
    "\n",
    "\n",
    "### Boosting\n",
    "\n",
    "La última técnica que vamos a describir en este capítulo es el _boosting_,\n",
    "que también servirá para mejorar los resultados de predicción de los\n",
    "árboles de decisión. Al igual que en la técnica de  _bagging_, el\n",
    "_boosting_ es un enfoque general que se puede aplicar a diversos métodos de\n",
    "aprendizaje automático para tareas de regresión o de clasificación.\n",
    "\n",
    "Cuando aplicamos _bagging_ creamos múltiples subconjuntos usando\n",
    "_bootsrapping_ y entrenamos un árbol para cada uno de estos subconjuntos,\n",
    "luego combinamos todos los árboles para crear\n",
    "un modelo predictivo único. La técnica de _boosting_  funciona de una manera\n",
    "similar, excepto que en este caso los árboles crecen de manera  secuencial.\n",
    "La construcción de cada árbol se basa en información de los árboles construidos\n",
    "previamente. En _boosting_ no construimos diferentes conjuntos de\n",
    "entrenamiento usando _bootstrapping_ sino que cada árbol se ajusta sobre una\n",
    "versión modificada del conjunto de datos original.\n",
    "\n",
    "Se dice que la técnica de _boosting_ implica un  aprendizaje lento. Dado el\n",
    "estado del modelo actual a partir  de sus residuos. Para un número de árboles\n",
    "al que llamaremos $B$, agregamos predicciones de árboles en de manera\n",
    "iterativa. Es decir, agregamos un segundo árbol para mejorar el rendimiento\n",
    "del primer árbol o para intentar corregir el error del primer árbol, y así\n",
    "sucesivamente. El proceso a realizar consiste en que hacemos es restar la\n",
    "predicción del primer modelo multiplicada por una constante $0 <λ \\leq 1$ de\n",
    "los valores objetivo y luego, tomando estos valores como valor objetivo,\n",
    "ajustamos el segundo modelo, y así sucesivamente. Podemos verlo como: nuevos\n",
    " modelos que intentan corregir modelos anteriores / errores del modelo anterior.\n",
    "\n",
    "En este caso, los árboles que forman el modelo suelen ser bastante\n",
    "pequeños, poco profundos, comparando con las técnicas anteriores. Cuentan con\n",
    "con pocos  nodos terminales. Estos estan determinados por el parámetro $d$\n",
    "del  algoritmo.\n",
    "\n",
    "Ajustando árboles pequeños a los residuos de los árboles anteriormente\n",
    "entrenados, mejoramos lentamente en áreas donde hasta ahora  no funcionaba\n",
    "de manera precisa. El parámetro  $λ$ ralentiza el proceso, lo que permite\n",
    "crear más árboles que ataquen a los  residuos.\n",
    "\n",
    "El aumento de los árboles de clasificación se realiza de una manera similar\n",
    "pero un poco más compleja, y sus detalles no entran en el ámbito de este material.\n",
    "\n",
    "Existen tres parámetros que permiten ajustar esta técnica:\n",
    "\n",
    "1. El número de árboles $B$. A diferencia de las técnicas de _bagging_ y  de\n",
    "_random forest_, esta técnica puede llegar al sobreajuste (__overfitting__)\n",
    "sobre el conjunto de entrenamiento si $B$ es demasiado grande, aunque este\n",
    "tiende a ocurrir lentamente. Para seleccionar el mejor parámetro de $B$ se\n",
    "recomienda usar la validación cruzada.\n",
    "\n",
    "2. El parámetro de contracción $λ$, es un positivo de valor muy pequeño.\n",
    "Este controla la velocidad a la que aprende el método. Los valores típicos son 0\n",
    ".01 o 0.001, su elección final depende del problema. Si\n",
    "elegimos un valor muy pequeño para $λ$ normalmente necesitaremos\n",
    "seleccionar un valor muy grande de $B$ para lograr un buen rendimiento.\n",
    "\n",
    "3. El número de divisiones en cada árbol, $d$, lo que controla la\n",
    "complejidad del conjunto potenciado. Normalmente, usar $d = 1$ funciona bien,\n",
    "en cuyo caso cada árbol es un volquete, que consta de una sola división. En\n",
    "este  caso, el conjunto de columnas aumentadas se ajusta a un modelo aditivo,\n",
    "ya que cada término implica solo una variable única. De manera más general,\n",
    "la profundidad de interacción y los controla el orden de interacción del\n",
    "modelo impulsado, ya que $d$ divisiones pueden involucrar a la mayoría de $d$\n",
    "variables.\n",
    "\n",
    "\n",
    "\n",
    "### Determinar la importancia de las características\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Un ejemplo completo\n",
    "\n",
    "**NOTA:** aquí la meva idea seria fer un exemple de classificació comparant\n",
    "arbres simples amb alguna de les tècniques anteriors. Fer selecció dels\n",
    "millors parametres i mostrar com canvia el resultat amb el nombre d'estimadors.\n",
    "\n",
    "## Resumen\n",
    "\n",
    "\n",
    "* Comentar que tant en els arbres individuals com en les tècnicques\n",
    "d'agregació ne le cas de classif es pot donar %\n",
    "\n",
    "* En resumen, el  promedio de un conjunto de observaciones permite reducir la\n",
    "varianza.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Ejercicios"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}