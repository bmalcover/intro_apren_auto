{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas de Agregación\n",
    "\n",
    "\n",
    "Los árboles de predicción, al igual que todo modelo estadístico, sufren el\n",
    "problema de la relación de equilibrio entre el bias y la varianza. El término\n",
    " bias hace referencia a cuánto  se alejan las predicciones de un modelo respecto\n",
    " a los valores reales. La varianza hace referencia a cuánto varía el\n",
    "modelo  dependiendo del conjunto con el que realizamos el entrenamiento.\n",
    "Cuanto mayor sea la complejidad de un modelo, es posible adaptarlo al\n",
    "problema que se quiere resolver, reduciendo el bias y mejorando su capacidad\n",
    "de predicción. Sin embargo, es peligroso tener un modelo tan ajustado al\n",
    "conjunto de entrenamiento que presenta una alta varianza y es incapaz de adaptarse\n",
    "a nuevas observaciones, entonces es cuándo aparece el  problema del sobreajuste.\n",
    "El modelo ideal es aquel que consigue un buen equilibrio entre bias y varianza.\n",
    "\n",
    "Estos términos se pueden trasladar a los árboles de decisión vistos en la\n",
    "sección anterior. De manera general, los árboles pequeños no serán capaces de\n",
    "representar de manera correcta la relación existente entre las variables, por\n",
    "lo que tienden a tener un alto bias, pero poca varianza. En cambio, los árboles\n",
    "grandes se ajustan mucho a los datos de entrenamiento, por lo\n",
    "que tienen menor bias pero una alta varianza.\n",
    "\n",
    "Tal como se describe en la introducción del capítulo __bagging__, __random\n",
    "forests__ y __boosting__ son técnicas que utilizan los árboles de decisión como\n",
    "las piezas elementales para construir  modelos de predicción con mejores\n",
    "resultados, consiguiendo un mejor equilibrio entre el bias y la varianza.\n",
    "\n",
    "\n",
    "### Bagging\n",
    "**NOTA:** Traduir bagging? He vist recursos on ho tradueixen com Agregación de\n",
    "bootstrap o empaquetado, si es tradueix ho faria amb empaquetado\n",
    "\n",
    "\n",
    "Los árboles de decisión sufren de una gran varianza, esto significa que si\n",
    "dividimos los datos de entrenamiento en dos partes de manera aleatoria, y\n",
    "construimos un árbol de decisión para cada una de las dos mitades, los\n",
    "árboles resultantes que obtenemos podrían ser muy diferentes. Un\n",
    "procedimiento  con baja varianza producirá resultados similares si se aplica\n",
    "a distintos conjuntos de datos.\n",
    "\n",
    "\n",
    "La técnica que aquí se presenta, _bagging_, es un procedimiento diseñado para\n",
    "reducir la varianza de un método de aprendizaje estadístico. Dado un\n",
    "conjunto de $n$ observaciones independientes $Z_1, \\ldots, Z_n$ cada una de\n",
    "ellas con su propia varianza $ \\sigma^2$, la varianza de la media de estas\n",
    "observaciones $\\overline{Z}$ es  $ \\frac{\\sigma^2}{n}$. Por lo\n",
    "tanto, una forma natural de reducir la varianza y, por lo tanto, aumentar la\n",
    "precisión de la predicción de un método de aprendizaje es construir\n",
    "diferentes conjuntos de entrenamiento, construir un modelo de\n",
    "predicción separado usando cada uno de los conjuntos y realizar el promedio\n",
    "de las predicciones resultantes.\n",
    "\n",
    "Desgraciadamente, normalmente no tenemos suficientes datos para crear\n",
    "diferentes conjuntos de entrenamiento. Es por este motivo que se aplica la\n",
    "técnica de _bootstrap_: simularemos los diferentes conjuntos de\n",
    "entrenamiento necesarios para construir diferentes árboles y reducir la\n",
    "varianza del clasificador. Esto se consigue extrayendo repetidos subconjuntos\n",
    "de la muestra original. Estos subconjuntos deben extraerse\n",
    "utilizando un muestreo con reposición, de tal forma que algunos elementos no\n",
    "serán  seleccionados y otros lo podrán ser más de una vez en cada muestreo.\n",
    "(_referenciar al Hasting capítol 5_)\n",
    "\n",
    "Para aplicar la técnica de _bagging_ construiremos un conjunto de árboles de\n",
    "regresión usando diferentes conjuntos de entrenamiento con la técnica de\n",
    "_bootstrapping_ y daremos como resultado final el promedio de las\n",
    "predicciones individuales de cada uno de los árboles que forman el conjunto.\n",
    "Estos árboles crecen profundamente y  no se podan . Al promediar el\n",
    "resultado de cada uno de estos árboles evidentemente, se consigue reducir la\n",
    "varianza. Se ha demostrado que el _bagging_ proporciona mejoras  en la\n",
    "precisión de los resultado al combinar un gran número de árboles en un\n",
    "solo modelo.\n",
    "\n",
    "Hasta ahora, hemos descrito el procedimiento de _bagging_ en  el caso de\n",
    "querer solventar un problema aplicando una regresión. Esta técnica también\n",
    "es aplicable a problemas de clasificación, en este caso se\n",
    "toma la clase que es predicha con mayor frecuencia por los diferentes árboles,\n",
    "es decir usamos la moda de las predicciones.\n",
    "\n",
    "Según lo explicado anteriormente, sabemos que cada árbol\n",
    "individual tiene una alta varianza, pero un bajo sesgo ya que comom hemos\n",
    "comentado són árboles son profundos. Al obtener información agregada del\n",
    "conjunto  árboles conseguimos reducir la varianza.\n",
    "\n",
    "\n",
    "#### Cálculo de el error (_out-of-bag_)\n",
    "\n",
    "\n",
    "Cuando usamos la técnica de _bootstrapping_ existe una manera  sencilla de\n",
    "estimar el error del método sin necesidad de realizar validación cruzada\n",
    "o crear un subconjunto específico de validación.\n",
    "\n",
    "El hecho de  que los árboles se ajusten de forma repetida empleando muestras\n",
    "generadas por _bootstrapping_ conlleva que en promedio, cada árbol solamente\n",
    "usa alrededor de dos tercios de las observaciones originales. Al\n",
    "tercio restante se le llama _out-of-bag_ (OOB). Si para cada árbol construido\n",
    "se registran las observaciones empleadas, se puede predecir la respuesta de\n",
    "una observación haciendo uso de aquellos árboles en los que esa observación\n",
    "ha sido excluida (OOB) y promediándolos si queremos realizar una tarea de\n",
    "regresión o obteniendo la moda en el caso que queramos realizar una tarea de\n",
    "clasificación.\n",
    "\n",
    "Siguiendo este proceso, se pueden  obtener las predicciones para las todas\n",
    "las observaciones y con ellas calcular el conocido como _OOB-mean square error_\n",
    "para casos de regresión o el _OOB-classification error_ para árboles de\n",
    "clasificación. Como la variable respuesta de cada observación se predice\n",
    "empleando únicamente los árboles en cuyo ajuste no participó dicha\n",
    "observación,  este cálculo sirve como estimación del\n",
    "error del conjunto de test.  (**Depen de si s'ha explicat o no**) _De hecho, si\n",
    "el número de árboles es suficientemente alto, el OOB-error es prácticamente\n",
    "equivalente al leave-one-out cross-validation error_.  Esto evita\n",
    "tener  que recurrir al proceso de cross-validation para la optimización de\n",
    "los hiperparámetros.\n",
    "\n",
    "\n",
    "#### Un ejemplo de _bagging_\n",
    "\n",
    "Como hemos explicado anteriormente esta técnica es aplicable a diferentes\n",
    "métodoes de aprendizaje. En el paquete _sklearn.ensemble_ de Scikit\n",
    "encontramos las clases _BaggingClassifier_ y _BaggingRegressor_ que usan los\n",
    "árboles de decisión como técnica por defecto.\n",
    "\n",
    "Vamos a ver un ejemplo de regresión con esta técnica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_regression(n_samples=50, n_features=1,\n",
    "                       n_informative=1, n_targets=1, noise=10,\n",
    "                       random_state=0, shuffle=True)\n",
    "\n",
    "elem = np.linspace(np.min(X),np.max(X),150).reshape(-1, 1)\n",
    "\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=33)\n",
    "regressor.fit(X, y)\n",
    "\n",
    "predicciones = regressor.predict(elem)\n",
    "\n",
    "regr = BaggingRegressor(n_estimators=100, random_state=0)\n",
    "regr.fit(X, y)\n",
    "\n",
    "y_predict= regr.predict(elem)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plt.scatter(X, y);\n",
    "#plt.plot(elem, y_predict,label=\"bagging\");\n",
    "#plt.plot(elem, predicciones, label=\"regressor tree\")\n",
    "#plt.legend()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest\n",
    "\n",
    "Los _random forests_ son una técnica que al igual que la técnica de\n",
    "_bagging_ consiste en la agregación de diversos árboles para proporcionar una\n",
    "decisión conjunta creando diferentes conjuntos de entrenamiento  mediante\n",
    "_bootstrapping_. Esta nueva técnica que presentamos consigue una mejora de\n",
    "rendimiento respecto a la técnica  de _bagging_.\n",
    "\n",
    "La particularidad que diferencia a los _random forest_ a la técnica\n",
    "anteriormente mencionada es que al construir\n",
    "cada  uno de los árboles de decisión, cada vez que se realiza una división de\n",
    "un  nodo en un árbol, se elige de manera  aleatoria un subconjunto, $m$, de\n",
    "características. Este nueva división solo podrá ser realizada usando una\n",
    "característica que pertenezca al conjunto $m$. Cada vez que se realiza una nueva\n",
    "división se vuelve a generar este conjunto, el valor por defecto de $m \\simeq\n",
    "\\sqrt{p}$ donde $p$ es el número total de características.\n",
    "\n",
    "En resumen, al construir un _random forest_, el algoritmo no considerará la\n",
    "mayoría de las características disponibles en cada nueva división de un nodo\n",
    "del árbol. Esto permite construir árboles que serán muy diferentes entre si y\n",
    "también evitar que existan características que dominantes.\n",
    "\n",
    "Una de las problemáticas que surgen del uso de la técnica de _bagging_ es la\n",
    "similitud de todos los árboles construidos. Desafortunadamente, realizar\n",
    "el promedio de muchos valores altamente correlacionados no conduce a una gran\n",
    "reducción de la varianza como cuando promediamos muchos valores no\n",
    "correlacionados.\n",
    "Esto significa que la técnica de _bagging_ no proporcionará una gran\n",
    "reducción de la varianza comparando con la que tenemos con la construcción de\n",
    "un solo árbol. Los _random forest_ estan diseñados para  solventar esta problemática al\n",
    "considerar un subconjunto de todas las características disponibles en cada\n",
    "división.\n",
    "\n",
    "La principal diferencia entre _bagging_ y _random forest_ es  la elección del\n",
    "tamaño del subconjunto del predictor, si un _random forest_ se\n",
    "construye usando  $m = p$, equivale a realizar la técnica de\n",
    "_bagging_.\n",
    "\n",
    "#### Un ejemplo de\n",
    "\n",
    "### Boosting\n",
    "\n",
    "La última técnica que vamos a describir en este capítulo es el _boosting_,\n",
    "que también servirá para mejorar los resultados de predicción de los\n",
    "árboles de decisión y del _bagging_. Al igual que en la técnica de\n",
    "_bagging_, el _boosting_ es un enfoque general que se puede aplicar a\n",
    "diversos métodos de aprendizaje automático para tareas de regresión o de\n",
    "clasificación.\n",
    "\n",
    "Cuando aplicamos _bagging_ creamos múltiples subconjuntos usando\n",
    "_bootsrapping_ y entrenamos un árbol para cada uno de estos subconjuntos,\n",
    "luego combinamos todos los árboles para crear\n",
    "un modelo predictivo único. La técnica de _boosting_  funciona de una manera\n",
    "similar, excepto que en este caso los árboles crecen de manera  secuencial,\n",
    "por este motivo se dice que la técnica de _boosting_ implica un  aprendizaje\n",
    "lento, ya que la construcción de cada árbol se basa en información de los\n",
    "árboles construidos previamente. En _boosting_ no construimos diferentes\n",
    "conjuntos de entrenamiento usando _bootstrapping_ sino que cada árbol se\n",
    "ajusta sobre una versión modificada del conjunto de datos original.\n",
    "\n",
    "\n",
    "El proceso de construcción del modelo es un proceso iterativo, en el que se\n",
    "realizan $B$ pasos, tantos com el número de árboles que tendrá el\n",
    "modelo final. Cada muestra del conjunto  de  entrenamiento tendrá un peso\n",
    "asociado, en un principio todas tendrán el mismo. _A continuación explicaremos\n",
    "el proceso de _boosting_ para las tareas de clasificación, para realizar una\n",
    "regresión con esta técnica el proceso es muy similar._\n",
    "\n",
    "\n",
    "En primer lugar entrenamos un árbol, calculamos el error absoluto de este\n",
    "clasificador y reajustamos los pesos de las muestras del conjunto de\n",
    "entrenamiento. Las muestras mal clasificadas tendrán pesos mayores que\n",
    "aquellas que han sido bien clasificadas.\n",
    "\n",
    "En cada paso siguiente agregamos un nuevo árbol para intentar corregir el error\n",
    "del modelo obtenido hasta ese momento. Al calcular el índice Gini o la\n",
    "Entropia de una división deberemos tener en cuenta el peso de las\n",
    "observaciones, así en los nuevos clasificadores damos importancia a las\n",
    "observaciones que hasta ese momento no habían sido bien clasificadas. El\n",
    "final de este paso, es el mismo que en el primero: calculamos el error del\n",
    "modelo con la incorporación de este clasificador y reajustamos los pesos de\n",
    "las muestras del conjunto de entrenamiento.\n",
    "\n",
    "En el momento de realizar una predicción cada clasificador individual, cada\n",
    "árbol, contribuirá de manera ponderada a la decisión final con el objetivo de\n",
    "dar mayor influencia a los clasificadores más precisos.\n",
    "\n",
    "\n",
    "En este caso, los árboles que forman el modelo suelen ser bastante\n",
    "pequeños, poco profundos, comparando con las técnicas anteriores. Cuentan con\n",
    "con pocos  nodos terminales. Estos estan determinados por el parámetro $d$\n",
    "del  algoritmo.\n",
    "\n",
    "Existen tres parámetros que permiten ajustar esta técnica:\n",
    "\n",
    "1. El número de árboles $B$. A diferencia de las técnicas de _bagging_ y  de\n",
    "_random forest_, esta técnica puede llegar al sobreajuste (__overfitting__)\n",
    "sobre el conjunto de entrenamiento si $B$ es demasiado grande, aunque este\n",
    "tiende a ocurrir lentamente. Para seleccionar el mejor parámetro de $B$ se\n",
    "recomienda usar la validación cruzada.\n",
    "\n",
    "2. El ratio de aprendizaje $λ$. Es un número positivo de valor muy pequeño que\n",
    "controla la velocidad a la que aprende el método. Los valores típicos son 0\n",
    ".01 o 0.001, su elección final depende del problema. Si\n",
    "elegimos un valor muy pequeño para $λ$ normalmente necesitaremos\n",
    "seleccionar un valor muy grande de $B$ para lograr un buen rendimiento.\n",
    "\n",
    "3. El número de divisiones en cada árbol, $d$. Controla la\n",
    "complejidad del conjunto potenciado. Normalmente, usar $d = 1$ funciona bien,\n",
    "en cuyo caso cada árbol consta de tres únicos nodos, el nodo raíz y dos nodos\n",
    "hoja.\n",
    "\n",
    "#### Un ejemplo de\n",
    "\n",
    "\n",
    "### Determinar la importancia de las características\n",
    "\n",
    "**NOTA**: Discutir que i com posar-ho o si va a un dels capítols de final.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Un ejemplo completo\n",
    "\n",
    "**NOTA:** aquí la meva idea seria fer un exemple de classificació comparant\n",
    "arbres simples amb alguna de les tècniques anteriors. Fer selecció dels\n",
    "millors parametres i mostrar com canvia el resultat amb el nombre d'estimadors.\n",
    "\n",
    "\n",
    "\n",
    "## Resumen\n",
    "\n",
    "**NOTA:**\n",
    "\n",
    "* Comentar que tant en els arbres individuals com en les tècnicques\n",
    "d'agregació ne le cas de classif es pot donar %\n",
    "\n",
    "* En resumen, el  promedio de un conjunto de observaciones permite reducir la\n",
    "varianza.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Ejercicios"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}