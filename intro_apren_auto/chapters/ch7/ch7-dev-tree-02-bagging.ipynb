{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas de Agregación\n",
    "\n",
    "\n",
    "Los árboles de predicción, al igual que todo modelo estadístico, sufren el\n",
    "problema del equilibrio bias-varianza. Recordemos que el término bias hace\n",
    "referencia a cuánto  se alejan las predicciones de un modelo respecto a los\n",
    "valores reales. La varianza hace referencia a cuánto varía el\n",
    "modelo  dependiendo del conjunto con el que realizamos el entrenamiento.\n",
    "Cuanto mayor sea la complejidad de un modelo, se dispone de una mayor\n",
    "capacidad de adaptarlo al problema que se quiere resolver, reduciendo así el\n",
    "bias y mejorando su capacidad de predicción. Sin embargo, el peligro es\n",
    "el de tener un modelo tan ajustado al conjunto de entrenamiento que presenta\n",
    "una alta varianza y es incapaz de adaptarse a nuevas observaciones,\n",
    "entonces aparece el  problema del sobreajuste. El mejor modelo es aquel que\n",
    "consigue un buen equilibrio entre bias y varianza.\n",
    "\n",
    "Estos términos se pueden trasladar a los árboles de decisión vistos en la\n",
    "sección anterior. De manera general, los árboles pequeños serán capaces de\n",
    "representar de manera correcta la relación existente entre las variables, por\n",
    "lo que tienden a tener un alto bias, pero poca varianza. En cambio, los árboles\n",
    "grandes se ajustan mucho a los datos de entrenamiento, por lo\n",
    "que tienen menor bias pero una alta varianza.\n",
    "\n",
    "Tal como se describe en la introducción del capítulo _Bagging_, _random\n",
    "forests_ y _boosting_ utilizan los árboles de decisión como las piezas\n",
    "elementales para construir mejores modelos de predicción consiguiendo un\n",
    "mejor equilibrio entre bias y varianza.\n",
    "\n",
    "\n",
    "### Bagging\n",
    "**NOTA:** Traduir bagging? Wikipedia ho tradueix com Agregación de bootstrap o\n",
    "empaquetado\n",
    "\n",
    "\n",
    "Los árboles de decisión sufren de una gran varianza, esto significa que si\n",
    "dividimos los datos de entrenamiento en dos partes de manera aleatoria, y\n",
    "construimos un árbol de decisión para cada una de las dos mitades, los\n",
    "árboles resultantes que obtenemos podrían ser muy diferentes. Un\n",
    "procedimiento  con baja varianza producirá resultados similares si se aplica\n",
    "a distintos conjuntos de datos.\n",
    "\n",
    "La técnica que aquí se presenta, __bagging__, es un procedimiento diseñado para\n",
    "reducir la varianza de un método de aprendizaje estadístico. Dado un\n",
    "conjunto de $n$ observaciones independientes $Z_1, \\ldots, Z_n$ cada una de\n",
    "ellas con su propia varianza $ \\sigma^2$, la varianza de la media de estas\n",
    "observaciones $\\overline{Z}$ es  $ \\frac{\\sigma^2}{n}$. Por lo\n",
    "tanto, una forma natural de reducir la varianza y, por lo tanto, aumentar la\n",
    "precisión de la predicción de un método de aprendizaje es construir\n",
    "diferentes conjuntos de entrenamiento, construir un modelo de\n",
    "predicción separado usando cada uno de los conjuntos y realizar el promedio\n",
    "de las predicciones resultantes.\n",
    "\n",
    "Desgraciadamente, normalmente no tenemos suficientes datos para crear\n",
    "diferentes conjuntos de entrenamiento. Es por este motivo que se aplica la\n",
    "técnica de __bootstrap__, simularemos los diferentes conjuntos de\n",
    "entrenamiento necesarios para construir diferentes árboles y reducir la\n",
    "varianza del clasificador. Esto se consigue extrayendo repetidos subconjuntos\n",
    "de la muestra original. Estos subconjuntos muestrales deben extraerse\n",
    "utilizando  un muestreo con reposición, de tal forma que algunos elementos no\n",
    "serán  seleccionados y otros lo podrán ser más de una vez en cada muestreo.\n",
    "(_refereciar al Hasting capítol 5_)\n",
    "\n",
    "**TODO**: gràfic mostrant bootstrap\n",
    "\n",
    "Para aplicar la técnica de _bagging_ construiremos un conjunto de árboles de\n",
    "regresión usando diferentes conjuntos de entrenamiento con la técnica\n",
    "explicada anteriormente y daremos como resultado final el promedio de las\n",
    "predicciones individuales de cada uno de los árboles que forman el conjunto.\n",
    "(_Estos árboles crecen profundamente y  no se podan_) . Al promediar el\n",
    "resultado de cada uno de estos árboles evidentemente, se consigue reducir la\n",
    "varianza. Se ha demostrado que el _bagging_ proporciona grandes mejoras  en la\n",
    "precisión de los resultado al combinar un gran número de árboles en un\n",
    "solo procedimiento.\n",
    "\n",
    "Hasta ahora, hemos descrito el procedimiento de _bagging_ en  el caso de\n",
    "necesitar solventar un problema aplicando una regresión. Esta técnica también\n",
    "es aplicable a problemas de clasificación, en este caso se\n",
    "toma la clase que más clasificadores (árboles) hayan predicho.\n",
    "\n",
    "**NOTA:** Integrar això, potser s'hauria de'eXplicar en la secció anterior.\n",
    "Bias-variance tradeoff\n",
    "\n",
    "_Según lo explicado anteriormente, sabemos que cada árbol\n",
    "individual tiene una alta varianza, pero un bajo sesgo (xq son profundos). Al\n",
    "obtener información agregada del conjunto  árboles se reduce la varianza._\n",
    "\n",
    "\n",
    "#### Cálculo de el error (_out-of-bag_)\n",
    "\n",
    "\n",
    "Cuando usamos la técnica de _bootstrapping_ existe una manera  sencilla de\n",
    "estimar el error del método sin necesidad de realizar validación cruzada\n",
    "o crear un subconjuto de validación.\n",
    "\n",
    "El hecho de  que los árboles se ajusten de forma repetida empleando muestras\n",
    "generadas por _bootstrapping_ conlleva que, en promedio, cada árbol solamente\n",
    "usa alrededor de dos tercios de las observaciones originales. Al\n",
    "tercio restante se le llama _out-of-bag_ (OOB). Si para cada árbol construido\n",
    "se registran las observaciones empleadas, se puede predecir la respuesta de\n",
    "una observación haciendo uso de aquellos árboles en los que esa observación\n",
    "ha sido excluida (OOB) y promediándolos si queremos realizar una tarea de\n",
    "regresión o obteniendo la moda en el caso que queramos realizar una tarea de\n",
    "clasificación.\n",
    "\n",
    "Siguiendo este proceso, se pueden  obtener las predicciones para las todas\n",
    "las observaciones y con ellas calcular el conocido como _OOB-mean square error_\n",
    "para casos de regresión o el _OOB-classification error_ para árboles de\n",
    "clasificación. Como la variable respuesta de cada observación se predice\n",
    "empleando únicamente los árboles en cuyo ajuste no participó dicha\n",
    "observación,  este cálculo sirve como estimación del\n",
    "error del conjunto de test.  (**Depen de si s'ha explicat o no**) _De hecho, si\n",
    "el número de árboles es suficientemente alto, el OOB-error es prácticamente\n",
    "equivalente al leave-one-out cross-validation error_.  Esto evita\n",
    "tener  que recurrir al proceso de cross-validation para la optimización de\n",
    "los hiperparámetros.\n",
    "\n",
    "\n",
    "#### Un ejemplo de _bagging_\n",
    "\n",
    "Como hemos explicado anteriormente esta técnica es aplicable a diferentes\n",
    "métodoes de aprendizaje. En el paquete _sklearn.ensemble_ de Scikit\n",
    "encontramos las clases _BaggingClassifier_ y _BaggingRegressor_ que usan los\n",
    "árboles de decisión como técnica por defecto.\n",
    "\n",
    "Vamos a ver un ejemplo de regresión con esta técnica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_regression(n_samples=50, n_features=1,\n",
    "                       n_informative=1, n_targets=1, noise=10,\n",
    "                       random_state=0, shuffle=True)\n",
    "\n",
    "elem = np.linspace(np.min(X),np.max(X),150).reshape(-1, 1)\n",
    "\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=33)\n",
    "regressor.fit(X, y)\n",
    "\n",
    "predicciones = regressor.predict(elem)\n",
    "\n",
    "regr = BaggingRegressor(n_estimators=100, random_state=0)\n",
    "regr.fit(X, y)\n",
    "\n",
    "y_predict= regr.predict(elem)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plt.scatter(X, y);\n",
    "#plt.plot(elem, y_predict,label=\"bagging\");\n",
    "#plt.plot(elem, predicciones, label=\"regressor tree\")\n",
    "#plt.legend()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest\n",
    "\n",
    "Los _random forests_ son una técnica que al igual que la técnica de\n",
    "_bagging_ consiste en la agregación de diversos árboles de decisión\n",
    "para proporcionar una decisión conjunta creando diferentes conjuntos de\n",
    "entrenamiento  mediante _bootstrapping_. Esta nueva técnica que presentamos\n",
    "consigue una mejora de rendimiento respecto a la técnica  de _bagging_.\n",
    "\n",
    "La particularidad que diferencia a los _random forest_ es que al construir\n",
    "cada  uno de los árboles de decisión, cada vez que se realiza una división de\n",
    "un  nodo en un árbol, se elige de manera  aleatoria un subconjunto, $m$, de\n",
    "características. Este nueva división solo podrá ser realizada usando una\n",
    "característica que pertenezca a $m$. Cada vez que se realiza una nueva\n",
    "división se vuelve a generar este conjunto, el valor por defecto de $m \\simeq\n",
    "\\sqrt{p}$ donde $p$ es el número total de características.\n",
    "\n",
    "\n",
    "En resumen, al construir un _random forest_, el algoritmo no considerará la\n",
    "mayoría de las características disponibles en cada nueva división de un nodo\n",
    "del árbol. Esto permite construir árboles que serán muy diferentes entre si y\n",
    " también evitar que existan características que dominantes.\n",
    "\n",
    "Una de las problemáticas que surgen del uso de la técnica de _bagging_ es la\n",
    "similitud de todos los árboles construidos. Desafortunadamente, realizar\n",
    "promedio de muchos valores altamente correlacionados no conduce a una gran\n",
    "reducción de la varianza como promediar muchos valores no correlacionados.\n",
    "Esto significa que la técnica de _bagging_ no proporcionará una gran\n",
    "reducción de la varianza comparando con la que tenemos con un solo árbol.\n",
    "Los _random forest_ estan diseñados para  solventar esta problemática al\n",
    "forzar  cada división a considerar un subconjunto de todas las\n",
    "características disponibles.\n",
    "\n",
    "La principal diferencia entre _bagging_ y _random forest_ es  la elección del\n",
    "tamaño del subconjunto del predictor, si un  _random forest_ se\n",
    "construye usando  $m = p$, esto equivale a realizar la técnica de\n",
    "_bagging_.\n",
    "\n",
    "\n",
    "### Boosting\n",
    "\n",
    "La última técnica que vamos a describir en este capítulo es el _boosting_,\n",
    "que también servirá para mejorar los resultados de predicción de los\n",
    "árboles de decisión. Al igual que en la técnica de  _bagging_, el\n",
    "_boosting_ es un enfoque general que se puede aplicar a diversos métodos de\n",
    "aprendizaje automático para tareas de regresión o de clasificación.\n",
    "\n",
    "Cuando aplicamos _bagging_ creamos múltiples subconjuntos usando\n",
    "_bootsrapping_ y entrenamos un árbol para cada uno de estos subconjuntos,\n",
    "luego combinamos todos los árboles para crear\n",
    "un modelo predictivo único. La técnica de _boosting_  funciona de una manera\n",
    "similar, excepto que en este caso los árboles crecen de manera  secuencial.\n",
    "La construcción de cada árbol se basa en información de los árboles construidos\n",
    "previamente. En _boosting_ no construimos diferentes conjuntos de\n",
    "entrenamiento usando _bootstrapping_ sino que cada árbol se ajusta sobre una\n",
    "versión modificada del conjunto de datos original.\n",
    "\n",
    "Se dice que la técnica de _boosting_ implica un  aprendizaje lento. Dado el\n",
    "estado del modelo actual a partir  de sus residuos. Para un número de árboles\n",
    "al que llamaremos $B$, agregamos predicciones de árboles en de manera\n",
    "iterativa. Es decir, agregamos un segundo árbol para mejorar el rendimiento\n",
    "del primer árbol o para intentar corregir el error del primer árbol, y así\n",
    "sucesivamente. El proceso a realizar consiste en que hacemos es restar la\n",
    "predicción del primer modelo multiplicada por una constante $0 <λ \\leq 1$ de\n",
    "los valores objetivo y luego, tomando estos valores como valor objetivo,\n",
    "ajustamos el segundo modelo, y así sucesivamente. Podemos verlo como: nuevos\n",
    " modelos que intentan corregir modelos anteriores / errores del modelo anterior.\n",
    "\n",
    "En este caso, los árboles que forman el modelo suelen ser bastante\n",
    "pequeños, poco profundos, comparando con las técnicas anteriores. Cuentan con\n",
    "con pocos  nodos terminales. Estos estan determinados por el parámetro $d$\n",
    "del  algoritmo.\n",
    "\n",
    "Ajustando árboles pequeños a los residuos de los árboles anteriormente\n",
    "entrenados, mejoramos lentamente en áreas donde hasta ahora  no funcionaba\n",
    "de manera precisa. El parámetro  $λ$ ralentiza el proceso, lo que permite\n",
    "crear más árboles que ataquen a los  residuos.\n",
    "\n",
    "El aumento de los árboles de clasificación se realiza de una manera similar\n",
    "pero un poco más compleja, y sus detalles no entran en el ámbito de este material.\n",
    "\n",
    "Existen tres parámetros que permiten ajustar esta técnica:\n",
    "\n",
    "1. El número de árboles $B$. A diferencia de las técnicas de _bagging_ y  de\n",
    "_random forest_, esta técnica puede llegar al sobreajuste (__overfitting__)\n",
    "sobre el conjunto de entrenamiento si $B$ es demasiado grande, aunque este\n",
    "tiende a ocurrir lentamente. Para seleccionar el mejor parámetro de $B$ se\n",
    "recomienda usar la validación cruzada.\n",
    "\n",
    "2. El parámetro de contracción $λ$, es un positivo de valor muy pequeño.\n",
    "Este controla la velocidad a la que aprende el método. Los valores típicos son 0\n",
    ".01 o 0.001, su elección final depende del problema. Si\n",
    "elegimos un valor muy pequeño para $λ$ normalmente necesitaremos\n",
    "seleccionar un valor muy grande de $B$ para lograr un buen rendimiento.\n",
    "\n",
    "3. El número de divisiones en cada árbol, $d$, lo que controla la\n",
    "complejidad del conjunto potenciado. Normalmente, usar $d = 1$ funciona bien,\n",
    "en cuyo caso cada árbol es un volquete, que consta de una sola división. En\n",
    "este  caso, el conjunto de columnas aumentadas se ajusta a un modelo aditivo,\n",
    "ya que cada término implica solo una variable única. De manera más general,\n",
    "la profundidad de interacción y los controla el orden de interacción del\n",
    "modelo impulsado, ya que $d$ divisiones pueden involucrar a la mayoría de $d$\n",
    "variables.\n",
    "\n",
    "\n",
    "\n",
    "### Determinar la importancia de las características\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Un ejemplo completo\n",
    "\n",
    "**NOTA:** aquí la meva idea seria fer un exemple de classificació comparant\n",
    "arbres simples amb alguna de les tècniques anteriors. Fer selecció dels\n",
    "millors parametres i mostrar com canvia el resultat amb el nombre d'estimadors.\n",
    "\n",
    "## Resumen\n",
    "\n",
    "\n",
    "* Comentar que tant en els arbres individuals com en les tècnicques\n",
    "d'agregació ne le cas de classif es pot donar %\n",
    "\n",
    "* En resumen, el  promedio de un conjunto de observaciones permite reducir la\n",
    "varianza.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Ejercicios"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}